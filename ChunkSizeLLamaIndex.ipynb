{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15156afd-3725-4cbd-b799-3566d0ace0f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install llama-index-core llama-index-readers-file llama-index-llms-ollama llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4e3442-fdfa-444f-b37e-416b541417b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58cf2a2-4ad8-4597-919e-57e1476fea9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install llama-index-embeddings-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df58b8b5-609c-41cb-8759-372e50fc8e5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b68fd44-4530-4cc8-b231-599093a54350",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71d4cdbe-7e2f-4da9-a8f0-e48d1181a777",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import openai\n",
    "import time\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    ")\n",
    "from llama_index.core.evaluation import ( DatasetGenerator, FaithfulnessEvaluator,  RelevancyEvaluator)\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f2b88e7-88a7-4182-8dcc-b498a0694870",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.readers.file import (\n",
    "    PandasCSVReader,\n",
    "    CSVReader\n",
    ")\n",
    "# CSV Reader Llamaindex\n",
    "parser = CSVReader()\n",
    "file_extractor = {\".csv\": parser}  # Add other CSV formats as needed\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"./documenti\", file_extractor=file_extractor\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5ceaf5-b439-4fed-862e-3a10942eb0a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e63256a-69fd-4fd7-a2de-7b6af3f23e88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llama = Ollama(model=\"llama3.1\", request_timeout=120.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b872fbe1-a709-4ac5-bc57-3dc09638b459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "emb_ollama = OllamaEmbedding(model_name=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc6773a-05e7-45c3-9d52-21ef7065d7f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import FaithfulnessEvaluator, RelevancyEvaluator\n",
    "# Per valutare le dimensioni di ciascun chunk, genereremo prima un insieme di 40 domande dalle prime 20 pagine..\n",
    "eval_documents = documents[:20]\n",
    "data_generator = DatasetGenerator.from_documents(documents, llm=llama)\n",
    "eval_questions = data_generator.generate_questions_from_nodes(num = 20)\n",
    "\n",
    "\n",
    "# Definire gli Evaluatori di Accuratezza e Rilevanza basati su LLAMA\n",
    "faithfulness_llama = FaithfulnessEvaluator(llm=llama)\n",
    "relevancy_llama = RelevancyEvaluator(llm=llama)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e982436-8879-46b1-818b-1a12e7cce4cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239ee312-c782-4eee-8c65-34bfd4b5f3fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NUOVO METODO DELLA LIBRERIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494d27f8-b71b-4ad4-86b3-a1f8d2ca2bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset.generator import RagDatasetGenerator , LabelledRagDataset\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.evaluation import FaithfulnessEvaluator, RelevancyEvaluator\n",
    "\n",
    "# Supponendo che `documents` e `llama` siano già definiti\n",
    "\n",
    "# Utilizzare le prime 20 pagine per la valutazione\n",
    "eval_documents = documents[:20]\n",
    "\n",
    "# Generare un set di 40 domande dalle prime 20 pagine utilizzando il nuovo RagDatasetGenerator\n",
    "#data_generator = RagDatasetGenerator.from_documents(documents, llm=llama)\n",
    "#eval_questions = data_generator.generate_questions_from_nodes(num=40)\n",
    "\n",
    "dataset_generator = RagDatasetGenerator.from_documents(\n",
    "    eval_documents,\n",
    "    llm=llama,\n",
    "    num_questions_per_chunk=2,  # set the number of questions per nodes\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "rag_dataset = dataset_generator.generate_dataset_from_nodes()\n",
    "\n",
    "\n",
    "# Definire gli Evaluatori di Accuratezza e Rilevanza basati su LLAMA\n",
    "faithfulness_llama = FaithfulnessEvaluator(llm=llama)\n",
    "relevancy_llama = RelevancyEvaluator(llm=llama)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a277d19-329e-46a7-ac26-991484e09fe8",
   "metadata": {},
   "source": [
    "CHUNK OVERLAP = A 10 PERCHE'\n",
    "- Bilanciamento tra continuità e ridondanza: Un overlap troppo piccolo potrebbe causare perdita di contesto tra i chunk. Un overlap troppo grande aumenterebbe eccessivamente la ridondanza e il costo computazionale.\n",
    "\n",
    "\n",
    "- Mantenimento del contesto: Dividendo per 10, si ottiene un overlap che di solito include abbastanza testo per mantenere il contesto tra i chunk adiacenti.\n",
    "\n",
    "\n",
    "- Efficienza computazionale: Questo rapporto offre un buon compromesso tra la qualità dei risultati e l'efficienza computazionale.\n",
    "\n",
    "\n",
    "- Adattabilità a diverse lunghezze di chunk: Questa regola si adatta bene a varie dimensioni di chunk, da quelli più piccoli a quelli più grandi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5740513-7b54-4a11-8c63-7fc83320df02",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### VALUTO IL CHUNK SIZE E CREO IL DATA FRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e204245a-0e7d-40c9-b61e-2e2617aa1b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Define the function to calculate average response time, average faithfulness, and average relevancy metrics\n",
    "def evaluate_response_time_and_accuracy(chunk_size):\n",
    "    total_response_time = 0\n",
    "    total_faithfulness = 0\n",
    "    total_relevancy = 0\n",
    "\n",
    "    \n",
    "    vector_index = VectorStoreIndex.from_documents(\n",
    "        eval_documents, llm=llama, embed_model=emb_ollama, chunk_size=chunk_size, chunk_overlap=chunk_size // 10\n",
    "    )\n",
    "\n",
    "    query_engine = vector_index.as_query_engine()\n",
    "    num_questions = len(eval_questions)\n",
    "\n",
    "    for question in eval_questions:\n",
    "        start_time = time.time()\n",
    "        response_vector = query_engine.query(question)\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        faithfulness_result = faithfulness_llama.evaluate_response(\n",
    "            response=response_vector\n",
    "        ).passing\n",
    "\n",
    "        relevancy_result = relevancy_llama.evaluate_response(\n",
    "            query=question, response=response_vector\n",
    "        ).passing\n",
    "\n",
    "        total_response_time += elapsed_time\n",
    "        total_faithfulness += faithfulness_result\n",
    "        total_relevancy += relevancy_result\n",
    "\n",
    "    average_response_time = total_response_time / num_questions\n",
    "    average_faithfulness = total_faithfulness / num_questions\n",
    "    average_relevancy = total_relevancy / num_questions\n",
    "\n",
    "    return average_response_time, average_faithfulness, average_relevancy\n",
    "\n",
    "# Prepare a list to store the results for each chunk size\n",
    "results = []\n",
    "\n",
    "# Iterate over different chunk sizes to evaluate the metrics\n",
    "for chunk_size in [128, 256, 512, 1024, 2048]:\n",
    "    avg_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size)\n",
    "    print(f\"Chunk size {chunk_size} - Average Response time: {avg_time:.2f}s, Average Faithfulness: {avg_faithfulness:.2f}, Average Relevancy: {avg_relevancy:.2f}\")\n",
    "    \n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        \"Chunk Size\": chunk_size,\n",
    "        \"Average Response Time (s)\": avg_time,\n",
    "        \"Average Faithfulness\": avg_faithfulness,\n",
    "        \"Average Relevancy\": avg_relevancy\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c725ab6f-9ded-435c-95f0-d54ff3952459",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### NON CREA IL DF VECCHIO CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4fa8f31d-f1fa-40fe-b744-956afbcb4b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35919/3076112235.py:9: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(llm=llama,embed_model=emb_ollama, chunk_size=chunk_size, chunk_overlap =chunk_size//10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size 128 - Average Response time: 1.08s, Average Faithfulness: 0.95, Average Relevancy: 0.60\n",
      "Chunk size 256 - Average Response time: 1.09s, Average Faithfulness: 0.95, Average Relevancy: 0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [23:02<23:02, 1382.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size 512 - Average Response time: 1.28s, Average Faithfulness: 0.95, Average Relevancy: 0.90\n",
      "Chunk size 1024 - Average Response time: 1.78s, Average Faithfulness: 0.90, Average Relevancy: 0.75\n",
      "Chunk size 2048 - Average Response time: 5.05s, Average Faithfulness: 0.70, Average Relevancy: 0.90\n"
     ]
    }
   ],
   "source": [
    "# Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size\n",
    "def evaluate_response_time_and_accuracy(chunk_size):\n",
    "    total_response_time = 0\n",
    "    total_faithfulness = 0\n",
    "    total_relevancy = 0\n",
    "\n",
    "    vector_index = VectorStoreIndex.from_documents(\n",
    "        eval_documents, llm=llama,embed_model=emb_ollama, chunk_size=chunk_size, chunk_overlap =chunk_size//10\n",
    "    )\n",
    "\n",
    "    query_engine = vector_index.as_query_engine()\n",
    "    num_questions = len(eval_questions)\n",
    "\n",
    "    for question in eval_questions:\n",
    "        start_time = time.time()\n",
    "        response_vector = query_engine.query(question)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        faithfulness_result = faithfulness_llama.evaluate_response(\n",
    "            response=response_vector\n",
    "        ).passing\n",
    "        \n",
    "        relevancy_result = relevancy_llama.evaluate_response(\n",
    "            query=question, response=response_vector\n",
    "        ).passing\n",
    "\n",
    "        total_response_time += elapsed_time\n",
    "        total_faithfulness += faithfulness_result\n",
    "        total_relevancy += relevancy_result\n",
    "\n",
    "    average_response_time = total_response_time / num_questions\n",
    "    average_faithfulness = total_faithfulness / num_questions\n",
    "    average_relevancy = total_relevancy / num_questions\n",
    "\n",
    "    return average_response_time, average_faithfulness, average_relevancy\n",
    "\n",
    "# Iterate over different chunk sizes to evaluate the metrics to help fix the chunk size.\n",
    "for chunk_size in [128, 256, 512, 1024, 2048]:\n",
    "  avg_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size)\n",
    "  print(f\"Chunk size {chunk_size} - Average Response time: {avg_time:.2f}s, Average Faithfulness: {avg_faithfulness:.2f}, Average Relevancy: {avg_relevancy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e60878a-3750-40b2-ba3b-9415eae4ff09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chunk Size</th>\n",
       "      <th>Average Response Time (s)</th>\n",
       "      <th>Average Faithfulness</th>\n",
       "      <th>Average Relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>512</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1024</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2048</td>\n",
       "      <td>5.05</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Chunk Size  Average Response Time (s)  Average Faithfulness   \n",
       "0         128                       1.08                  0.95  \\\n",
       "1         256                       1.09                  0.95   \n",
       "2         512                       1.28                  0.95   \n",
       "3        1024                       1.78                  0.90   \n",
       "4        2048                       5.05                  0.70   \n",
       "\n",
       "   Average Relevancy  \n",
       "0               0.60  \n",
       "1               0.65  \n",
       "2               0.90  \n",
       "3               0.75  \n",
       "4               0.90  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Mostro i risultati ottenuti\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Risultati dei vari chunk size\n",
    "data = {\n",
    "    \"Chunk Size\": [128, 256, 512, 1024, 2048],\n",
    "    \"Average Response Time (s)\": [1.08, 1.09, 1.28, 1.78, 5.05],\n",
    "    \"Average Faithfulness\": [0.95, 0.95, 0.95, 0.90, 0.70],\n",
    "    \"Average Relevancy\": [0.60, 0.65, 0.90, 0.75, 0.90]\n",
    "}\n",
    "\n",
    "# Creare un DataFrame con i dati\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Visualizzare la tabella\n",
    "display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c108eaa2-4a58-45ca-9a9f-f95d4c98276b",
   "metadata": {},
   "source": [
    "## Analisi dei Dati\n",
    "\n",
    "### Dimensioni dei Chunk\n",
    "\n",
    "- **Dimensioni più piccole (128, 256)**: Offrono tempi di risposta più rapidi, ma la pertinenza è inferiore rispetto ai chunk size più grandi. La fedeltà rimane alta, ma la pertinenza non è altrettanto buona come con i chunk size maggiori.\n",
    "\n",
    "- **Dimensioni medie (512)**: Forniscono un buon compromesso con tempi di risposta moderati e la migliore pertinenza. La fedeltà è alta e la pertinenza è molto buona.\n",
    "\n",
    "- **Dimensioni più grandi (1024, 2048)**: Presentano tempi di risposta più lunghi, ma la pertinenza è molto alta, specialmente per il chunk size di 2048. Tuttavia, la fedeltà diminuisce per i chunk size più grandi.\n",
    "\n",
    "### Raccomandazione\n",
    "\n",
    "**Dimensione del Chunk di 512**: Questa dimensione sembra offrire un equilibrio ottimale tra il tempo di risposta e la qualità delle risposte. Fornisce un buon compromesso tra fedeltà e pertinenza senza un tempo di risposta eccessivamente lungo.\n",
    "\n",
    "**Dimensione del Chunk di 256**: Potrebbe essere una scelta valida se preferisci tempi di risposta leggermente più rapidi, mantenendo una qualità di risposta ancora accettabile.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
