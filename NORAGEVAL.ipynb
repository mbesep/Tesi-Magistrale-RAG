{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qsBvipA_nRW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import re\n",
    "\n",
    "# Esegui il comando 'pip freeze' e ottieni l'output\n",
    "result = subprocess.run(['pip', 'freeze'], stdout=subprocess.PIPE)\n",
    "packages = result.stdout.decode().splitlines()\n",
    "\n",
    "# Filtrare le linee che contengono \"file://\"\n",
    "filtered_packages = [package for package in packages if not re.search(r'file://', package)]\n",
    "\n",
    "# Scrivere i pacchetti filtrati nel file 'requirements.txt'\n",
    "with open('requirements.txt', 'w') as file:\n",
    "    for package in filtered_packages:\n",
    "        file.write(package + '\\n')\n",
    "\n",
    "print(\"Il file requirements.txt è stato creato senza stringhe 'file:///home/...'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CadTskQjB6pr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain chromadb langchain_community pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ToGu1ieU_nRY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNMAyO6r_nRZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1V9uh89_nRZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install langchain-groq==0.1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZO3t3XlA2KZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 461,
     "status": "ok",
     "timestamp": 1724077181550,
     "user": {
      "displayName": "Giuseppe Salcuni",
      "userId": "05750098578665397452"
     },
     "user_tz": -120
    },
    "id": "oTIELYa5B6pv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA, LLMChain\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 494,
     "status": "ok",
     "timestamp": 1724077421430,
     "user": {
      "displayName": "Giuseppe Salcuni",
      "userId": "05750098578665397452"
     },
     "user_tz": -120
    },
    "id": "pM9ahLgI_nRa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#imposto la chiavi per i servizi che utilizzerò\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "os.environ['LANGCHAIN_API_KEY'] = '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'\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"21a4eae201494ec581d92fbeb45891b3\" #Mbesi Open 9d7cdcf0ed72472dbf75f7e02a25d506 - # Mbesi est 21a4eae201494ec581d92fbeb45891b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV47mYTzBlOR",
    "tags": []
   },
   "source": [
    "# IMPOSTO AZURE OPENAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 493,
     "status": "ok",
     "timestamp": 1724081598546,
     "user": {
      "displayName": "Giuseppe Salcuni",
      "userId": "05750098578665397452"
     },
     "user_tz": -120
    },
    "id": "zG6AZzy9Bp25",
    "tags": []
   },
   "outputs": [],
   "source": [
    "azure_configs = {\n",
    "    \"base_url\": \"https://mbesiest.openai.azure.com/\", # Mbesi open https://mbesiopen.openai.azure.com/ - Mbesi est https://mbesiest.openai.azure.com/\n",
    "    \"model_deployment\": \"gpt-4o-mini\",\n",
    "    \"model_name\": \"gpt-4o-mini\",\n",
    "    \"embedding_deployment\": \"text-embedding-ada-002\",\n",
    "    \"embedding_name\": \"text-embedding-ada-002\",  # most likely  text-embedding-ada-002\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1228,
     "status": "ok",
     "timestamp": 1724081912005,
     "user": {
      "displayName": "Giuseppe Salcuni",
      "userId": "05750098578665397452"
     },
     "user_tz": -120
    },
    "id": "SubA0ZdzCDrZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import AzureChatOpenAI\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "from ragas import evaluate\n",
    "\n",
    "azure_model_GPT3 = AzureChatOpenAI(\n",
    "    openai_api_version=\"2024-06-01\",\n",
    "    azure_endpoint=azure_configs[\"base_url\"],\n",
    "    azure_deployment=\"gpt-35-turbo\",\n",
    "    model=\"gpt-35-turbo\",\n",
    "    validate_base_url=False,\n",
    ")\n",
    "azure_model_GPT4OMINI = AzureChatOpenAI(\n",
    "    openai_api_version=\"2024-06-01\",\n",
    "    azure_endpoint=azure_configs[\"base_url\"],\n",
    "    azure_deployment=azure_configs[\"model_deployment\"],\n",
    "    model=azure_configs[\"model_name\"],\n",
    "    validate_base_url=False,\n",
    ")\n",
    "\n",
    "# init the embeddings for answer_relevancy, answer_correctness and answer_similarity\n",
    "azure_embeddings = AzureOpenAIEmbeddings(\n",
    "    openai_api_version=\"2024-02-01\",\n",
    "    azure_endpoint=azure_configs[\"base_url\"],\n",
    "    azure_deployment=azure_configs[\"embedding_deployment\"],\n",
    "    model=azure_configs[\"embedding_name\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-mry09m_nRb",
    "tags": []
   },
   "source": [
    "# INPUT E CHUNK DEL FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34817,
     "status": "ok",
     "timestamp": 1724086371787,
     "user": {
      "displayName": "Giuseppe Salcuni",
      "userId": "05750098578665397452"
     },
     "user_tz": -120
    },
    "id": "dvN-sk1bYrj8",
    "outputId": "59a6450e-cecc-4183-e1f9-1fc71d028a7c"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMNEnQwC_nRd"
   },
   "source": [
    "NUOVO DATASET COMPLETO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mby16LN2B6px",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path='documenti/DomandeRisposteNew.csv', csv_args={\n",
    "    'delimiter': ';',\n",
    "    'quotechar': '\"',\n",
    "    'fieldnames': ['DOMANDE', 'RISPOSTE']\n",
    "})\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GY8CceUvZRso",
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 631,
     "status": "ok",
     "timestamp": 1724077196135,
     "user": {
      "displayName": "Giuseppe Salcuni",
      "userId": "05750098578665397452"
     },
     "user_tz": -120
    },
    "id": "eNN3HCNUB6py",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=51) #300 - 50/ 250 - 50/ 256 - 25/ 300 - 30\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 466,
     "status": "ok",
     "timestamp": 1724077199877,
     "user": {
      "displayName": "Giuseppe Salcuni",
      "userId": "05750098578665397452"
     },
     "user_tz": -120
    },
    "id": "H3emZZZFvPcI",
    "outputId": "a1d01f56-e617-4550-ab4a-b4dcc6609f3b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1724077689551,
     "user": {
      "displayName": "Giuseppe Salcuni",
      "userId": "05750098578665397452"
     },
     "user_tz": -120
    },
    "id": "7TKEoczMB6p0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RAGAS expects a file_name dict as key\n",
    "for document in chunks:\n",
    "    document.metadata['file_name'] = document.metadata['source']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nM_brQSAB6p1",
    "tags": []
   },
   "source": [
    "# CON LLM OPEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3kOH4vL_nRg",
    "tags": []
   },
   "source": [
    "## IMPORTO LIBRERIE - CREO VECTORSTORE, PROMPT E RETRIEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3610,
     "status": "ok",
     "timestamp": 1724077237808,
     "user": {
      "displayName": "Giuseppe Salcuni",
      "userId": "05750098578665397452"
     },
     "user_tz": -120
    },
    "id": "44ioWSpAB6p8",
    "outputId": "91a1e074-c52e-44f7-af9a-7f5e7e81ec94",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"GPU is\", \"available\" if torch.cuda.is_available() else \"NOT AVAILABLE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2iqZISbsAWw"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SE MI TROVO SU COLAB INSTALLO IL TERMINALE PER OLLAMA\n",
    "#!pip install colab-xterm\n",
    "%load_ext colabxterm\n",
    "%xterm\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8VzBmQ0YB6p3"
   },
   "outputs": [],
   "source": [
    "# scrivo le seguenti linee in entrambi i casi:\n",
    "#1- curl -faSL https://ollama.com/install.sh | sh\n",
    "#2- ollama serve & ollama pull gemma2\n",
    "#3- ollama pull nomic-embed-text\n",
    "#4- ollama pull llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blZXyEyxB6p5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importo le librerie che utilizzerò\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader # se carico direttamente una cartella di documenti\n",
    "#from langchain_openai import ChatOpenAI, OpenAIEmbeddings # se utilizzo openai\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbmEYstwB6p6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "#embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Simuliamo il tempo di esecuzione per la dimostrazione\n",
    "start_time = time.time()\n",
    "\n",
    "# Crea un oggetto tqdm per monitorare l'avanzamento\n",
    "with tqdm(total=100) as pbar:\n",
    "    # Simulazione dell'elaborazione (ad esempio per un loop sui chunks)\n",
    "    vectorstore = Chroma.from_documents(chunks, embeddings, persist_directory=\"./chroma_db/med256_db\")\n",
    "    \n",
    "    # Aggiornamento della barra di caricamento\n",
    "    pbar.update(100)\n",
    "\n",
    "# Calcolo del tempo di esecuzione\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Tempo di esecuzione: {execution_time:.2f} secondi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DavnRDIAB6p9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(chunks, embeddings, persist_directory=\"./chroma_db/med256_db\")\n",
    "#, persist_directory=\"./chroma_db/txt_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlTD06ru_nRh",
    "outputId": "80743968-7b54-46c1-8cde-bf1373e308f6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In caso ho l'ho già creato, carico database vettoriale\n",
    "vectorstore = Chroma(persist_directory=\"./chroma_db/med_db\", embedding_function = embeddings) #med256 per il vectorstore diviso in 256 chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSqHxfNJ_nRh",
    "outputId": "1bc04cef-633d-44ba-aafc-dedae0b9387d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dimostro che il caricamento è avvenuto correttamente\n",
    "docum = vectorstore.similarity_search(\"come curo l'ipertensione?\")\n",
    "print(docum[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RbSOQ32vB6p-",
    "outputId": "fccc7e23-56bd-46af-a4b5-029c32a14155",
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlJ6fP2XB6p-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={'k': 3}# ritorna i k risultati simili quando eseguo una ricerca sul documento\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xXVokccgB6qA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#PROMPT MIGLIORATO CON GIANLUCA\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Sei un assistente medico IA. Fornisci risposte accurate e dettagliate in italiano, usando un linguaggio chiaro ma appropriato. Rispetta sempre la privacy del paziente.\n",
    "Se non sei sicuro della risposta, dichiara \"Non sicuro della risposta\".\n",
    "\n",
    "\n",
    "Domanda: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"question\"]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Sei un assistente medico IA specializzato in ipertensione. Fornisci risposte dettagliate e basate su evidenze scientifiche, mantenendo un linguaggio chiaro e accessibile. Rispetta sempre la privacy del paziente e, se non sei sicuro della risposta, dichiara \"Non sicuro della risposta\". \n",
    "Includi le raccomandazioni attuali e spiega i concetti medici in modo comprensibile.\n",
    "\n",
    "Domanda: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"question\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXceEFQ-_nRi",
    "tags": []
   },
   "source": [
    "## IMPOSTO TUTTE LE TIPOLOGIE DI RETRIVER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zqRrpXl_nRi",
    "tags": []
   },
   "source": [
    "### BASE RETRIEVER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sE4dd1Yt_nRi",
    "tags": []
   },
   "source": [
    "Retriever base è utilizzato per eseguire una ricerca di somiglianza, trovando i documenti più simili alla query basandosi sui loro vettori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viRXDKPVB6qC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "rag_chain = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(rag_chain.invoke(\"Quali strumenti tecnologici mostrano dati storici e attuali dell'ipertensione?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever.invoke(\"Quali strumenti tecnologici mostrano dati storici e attuali dell'ipertensione? \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiL6nWIr_nRj"
   },
   "source": [
    "Il MultiQueryRetriever automatizza l'ottimizzazione dei prompt utilizzando un LLM per generare più query da diverse prospettive relative a una specifica richiesta dell'utente. Per ciascuna query, il sistema recupera un insieme di documenti rilevanti e poi unisce in modo unico i risultati ottenuti da tutte le query, al fine di ottenere un insieme più ampio di documenti potenzialmente utili. Generando diverse prospettive sulla stessa domanda, il MultiQueryRetriever può superare alcune limitazioni del reperimento basato sulla distanza, fornendo così un insieme di risultati più ricco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IaxcKcJZ_nRj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import MultiQueryRetriever\n",
    "final_retriever = MultiQueryRetriever.from_llm(retriever=retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yeig_RUA_nRn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "multirag_chain = (\n",
    "    {\"context\": final_retriever,  \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QPtJrtcd_nRo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(multirag_chain.invoke(\"Come si cura l'ipertensione?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oueZEXIdB6qC",
    "outputId": "17197149-a7bd-4125-f7b8-05250961ff41",
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator_llm = Ollama(model=\"mistral-nemo\")\n",
    "critic_llm = Ollama(model=\"qwen2\")\n",
    "\n",
    "ollama_emb = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\",\n",
    "    #model=\"mxbai-embed-large\"\n",
    ")\n",
    "\n",
    "print('Generator LLM: '+ generator_llm.invoke('Salutami in italiano'))\n",
    "print('Critic LLM: '+ critic_llm.invoke('Salutami in italiano e dimmi chi sei'))\n",
    "\n",
    "r2 = ollama_emb.embed_query(\n",
    "    \"dimmi la seconda lettera dell'alfabeto greco\"\n",
    ")\n",
    "\n",
    "print(f'Dimensione Embedding: {len(r2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## IMPORTO METRICHE RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "hSCMwHpx0NnF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics.critique import harmfulness\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_similarity,\n",
    "   #context_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_correctness\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bc5wCld7_nRq",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## VALUTAZIONE CON METRICHE RAGAS -BASE RETRIEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9UlXvaIz90p",
    "tags": []
   },
   "outputs": [],
   "source": [
    "questions = testset.to_pandas()[\"question\"].to_list()\n",
    "ground_truth = testset.to_pandas()[\"ground_truth\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5VneVbT_nRq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carica il CSV in un nuovo DataFrame\n",
    "loaded_df = pd.read_csv('test_dataChatGPT4O_FINALE.csv') #test_dataGPT4OMINI _BUONO / test_dataChatGPT4O_BUONO (sembra ottenere risultati migliori con gpt4O)\n",
    "\n",
    "# Estrai le colonne 'question' e 'ground_truth'\n",
    "questions = loaded_df[\"question\"].to_list()\n",
    "ground_truth = loaded_df[\"ground_truth\"].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-G451qhE_nRr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#PER CAMBIARE LLM DI VALUTAZIONE TUTTI I RETRIEVER FARANNO RIFERIMENTO\n",
    "llm = Ollama(model=\"echelonify/med-qwen2\") #echelonify/med-qwen2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQV_WERd_nRr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assicurati che questions e ground_truth siano definiti\n",
    "# questions = testset.to_pandas()[\"question\"].to_list()\n",
    "# ground_truth = testset.to_pandas()[\"ground_truth\"].to_list()\n",
    "\n",
    "data = {\"question\": [], \"answer\": [], \"contexts\": [], \"ground_truth\": ground_truth}\n",
    "\n",
    "# Utilizzo di tqdm per creare una barra di avanzamento\n",
    "for query in tqdm(questions, desc=\"Processing queries\"):\n",
    "    data[\"question\"].append(query)\n",
    "    data[\"answer\"].append(rag_chain.invoke(query))\n",
    "    data[\"contexts\"].append([\"\"])  # Contesto vuoto per ogni elemento\n",
    "\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98_9Gf07_nRr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset.to_pandas()\n",
    "datacompleto= dataset.to_pandas()\n",
    "datacompleto.to_csv('DataCompletoQWENMED5.csv', index=False)\n",
    "#datacompleto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#SE LO IMPORTO DAI DATAFRAME PRECEDENTI\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Leggi il file CSV in un DataFrame di Pandas\n",
    "dataframe = pd.read_csv('DataCompletoQWENMED.csv')\n",
    "\n",
    "# Converti il DataFrame in un dataset Hugging Face\n",
    "datasetImportato = Dataset.from_pandas(dataframe)\n",
    "\n",
    "# Mostra il dataset\n",
    "print(datasetImportato)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#PER TESTARE SINGOLA METRICA\n",
    "score = evaluate(dataset,metrics=[answer_relevancy, faithfulness, answer_correctness], llm=azure_model_GPT4OMINI, embeddings=azure_embeddings)\n",
    "score.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DatasetScore= score.to_pandas()\n",
    "DatasetScore.to_csv('SCOREQWENMED.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#score med qwen\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6qOjHMk29RK-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from ragas import evaluate\n",
    "\n",
    "#langchain_llm =  # any langchain LLM instance\n",
    "#langchain_embeddings = # any langchain Embeddings instance\n",
    "\n",
    "resultBase = evaluate(\n",
    "        dataset = dataset,\n",
    "        metrics=[\n",
    "              faithfulness,\n",
    "              answer_relevancy,\n",
    "              answer_correctness,\n",
    "              context_recall,\n",
    "              context_precision,\n",
    "              harmfulness\n",
    "          ],\n",
    "        llm=generator_llm,\n",
    "        embeddings=ollama_emb,\n",
    "        raise_exceptions=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resultBase.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPoMV_hy_nRs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assicurati che questions e ground_truth siano definiti\n",
    "# questions = testset.to_pandas()[\"question\"].to_list()\n",
    "# ground_truth = testset.to_pandas()[\"ground_truth\"].to_list()\n",
    "\n",
    "data = {\"question\": [], \"answer\": [], \"contexts\": [], \"ground_truth\": ground_truth}\n",
    "\n",
    "# Utilizzo di tqdm per creare una barra di avanzamento\n",
    "for query in tqdm(questions, desc=\"Processing queries\"):\n",
    "    data[\"question\"].append(query)\n",
    "    data[\"answer\"].append(multirag_chain.invoke(query))\n",
    "    data[\"contexts\"].append([doc.page_content for doc in retriever.get_relevant_documents(query)])\n",
    "\n",
    "response_dataset_advanced_retrieval = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHGNI6ZY_nRs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_dataset_advanced_retrieval.to_pandas()\n",
    "responseMultiRetriver= response_dataset_advanced_retrieval.to_pandas()\n",
    "responseMultiRetriver.to_csv('responseMultiRetriver.csv', index=False)\n",
    "responseMultiRetriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#PER TESTARE SINGOLA METRICA\n",
    "score2 = evaluate(response_dataset_advanced_retrieval,metrics=[answer_relevancy, faithfulness], llm=azure_model_GPT4OMINI, embeddings=azure_embeddings)\n",
    "score2.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gh2A3-dA_nRs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#VALUTO CON CHAT IL MULTIQUERYRETRIVER\n",
    "resultMulti = evaluate(\n",
    "        dataset = response_dataset_advanced_retrieval,\n",
    "        metrics=[\n",
    "              faithfulness,\n",
    "              answer_relevancy,\n",
    "              answer_correctness,\n",
    "              context_recall,\n",
    "              context_precision,\n",
    "              harmfulness\n",
    "          ],\n",
    "        llm=generator_llm,\n",
    "        embeddings=ollama_emb,\n",
    "        raise_exceptions=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resultMulti.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZUvLDul_nRt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assicurati che questions e ground_truth siano definiti\n",
    "# questions = testset.to_pandas()[\"question\"].to_list()\n",
    "# ground_truth = testset.to_pandas()[\"ground_truth\"].to_list()\n",
    "\n",
    "data = {\"question\": [], \"answer\": [], \"contexts\": [], \"ground_truth\": ground_truth}\n",
    "\n",
    "# Utilizzo di tqdm per creare una barra di avanzamento\n",
    "for query in tqdm(questions, desc=\"Processing queries\"):\n",
    "    data[\"question\"].append(query)\n",
    "    data[\"answer\"].append(ensemble_chain.invoke(query))\n",
    "    data[\"contexts\"].append([doc.page_content for doc in retriever.get_relevant_documents(query)])\n",
    "\n",
    "response_dataset_ensemble_retrieval = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#PER TESTARE SINGOLA METRICA\n",
    "score3 = evaluate(response_dataset_ensemble_retrieval,metrics=[answer_relevancy, faithfulness], llm=azure_model_GPT4OMINI, embeddings=azure_embeddings)\n",
    "score3.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxPee9vQ_nRt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#VALUTO CON CHAT IL ENSEMBLERETRIVER\n",
    "resultEnsemble = evaluate(\n",
    "        dataset = response_dataset_ensemble_retrieval,\n",
    "        metrics=[\n",
    "              faithfulness,\n",
    "              answer_relevancy,\n",
    "              answer_correctness,\n",
    "              context_recall,\n",
    "              context_precision,\n",
    "              harmfulness\n",
    "          ],\n",
    "        llm=generator_llm,\n",
    "        embeddings=ollama_emb,\n",
    "        raise_exceptions=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resultEnsemble.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resultBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resultMulti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resultEnsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOdoPCzS_nRt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "questions = testset.to_pandas()[\"question\"].to_list()\n",
    "ground_truth = testset.to_pandas()[\"ground_truth\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQyWNk1a_nRt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carica il CSV in un nuovo DataFrame\n",
    "loaded_df = pd.read_csv('test_dataLLAMA3.csv')\n",
    "\n",
    "# Estrai le colonne 'question' e 'ground_truth'\n",
    "questions = loaded_df[\"question\"].to_list()\n",
    "ground_truth = loaded_df[\"ground_truth\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X7BiYTD5_nRt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#PER CAMBIARE LLM DI VALUTAZIONE TUTTI I RETRIEVER FARANNO RIFERIMENTO\n",
    "llm = Ollama(model=\"llama3.1:70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNliii_5_nRu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assicurati che questions e ground_truth siano definiti\n",
    "# questions = testset.to_pandas()[\"question\"].to_list()\n",
    "# ground_truth = testset.to_pandas()[\"ground_truth\"].to_list()\n",
    "\n",
    "# Inizializzazione dei tre dizionari\n",
    "dataset = {\"question\": [], \"answer\": [], \"contexts\": [], \"ground_truth\": ground_truth}\n",
    "response_dataset_advanced_retrieval = {\"question\": [], \"answer\": [], \"contexts\": [], \"ground_truth\": ground_truth}\n",
    "response_dataset_ensemble_retrieval = {\"question\": [], \"answer\": [], \"contexts\": [], \"ground_truth\": ground_truth}\n",
    "\n",
    "# Utilizzo di tqdm per creare una barra di avanzamento\n",
    "for query in tqdm(questions, desc=\"Processing queries\"):\n",
    "    # Recupero dei documenti rilevanti (assumendo che sia lo stesso per tutte le catene)\n",
    "    relevant_docs = retriever.get_relevant_documents(query)\n",
    "    contexts = [doc.page_content for doc in relevant_docs]\n",
    "\n",
    "    # Popolare il dataset originale\n",
    "    dataset[\"question\"].append(query)\n",
    "    dataset[\"answer\"].append(rag_chain.invoke(query))\n",
    "    dataset[\"contexts\"].append(contexts)\n",
    "\n",
    "    # Popolare il dataset per advanced retrieval\n",
    "    response_dataset_advanced_retrieval[\"question\"].append(query)\n",
    "    response_dataset_advanced_retrieval[\"answer\"].append(multirag_chain.invoke(query))\n",
    "    response_dataset_advanced_retrieval[\"contexts\"].append(contexts)\n",
    "\n",
    "    # Popolare il dataset per ensemble retrieval\n",
    "    response_dataset_ensemble_retrieval[\"question\"].append(query)\n",
    "    response_dataset_ensemble_retrieval[\"answer\"].append(ensemble_chain.invoke(query))\n",
    "    response_dataset_ensemble_retrieval[\"contexts\"].append(contexts)\n",
    "\n",
    "# Convertire i dizionari in Dataset\n",
    "dataset = Dataset.from_dict(dataset)\n",
    "response_dataset_advanced_retrieval = Dataset.from_dict(response_dataset_advanced_retrieval)\n",
    "response_dataset_ensemble_retrieval = Dataset.from_dict(response_dataset_ensemble_retrieval)\n",
    "\n",
    "# Convertire in DataFrame se necessario\n",
    "dataset_df = dataset.to_pandas()\n",
    "advanced_retrieval_df = response_dataset_advanced_retrieval.to_pandas()\n",
    "ensemble_retrieval_df = response_dataset_ensemble_retrieval.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-gHD1aG-_nRu"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from ragas import evaluate\n",
    "\n",
    "#langchain_llm =  # any langchain LLM instance\n",
    "#langchain_embeddings = # any langchain Embeddings instance\n",
    "\n",
    "resultBase = evaluate(\n",
    "        dataset = dataset,\n",
    "        metrics=[\n",
    "              faithfulness,\n",
    "              answer_relevancy,\n",
    "              answer_correctness#,\n",
    "              #context_recall,\n",
    "              #context_precision\n",
    "              #context_relevancy\n",
    "          ],\n",
    "        llm=generator_llm,\n",
    "        embeddings=ollama_emb,\n",
    "        raise_exceptions=False\n",
    "    )\n",
    "\n",
    "resultMulti = evaluate(\n",
    "        dataset = response_dataset_advanced_retrieval,\n",
    "        metrics=[\n",
    "              faithfulness,\n",
    "              answer_relevancy,\n",
    "              answer_correctness#,\n",
    "              #context_recall,\n",
    "              #context_precision\n",
    "              #context_relevancy\n",
    "          ],\n",
    "        llm=generator_llm,\n",
    "        embeddings=ollama_emb,\n",
    "        raise_exceptions=False\n",
    "    )\n",
    "\n",
    "resultEnsemble = evaluate(\n",
    "        dataset = response_dataset_ensemble_retrieval,\n",
    "        metrics=[\n",
    "              faithfulness,\n",
    "              answer_relevancy,\n",
    "              answer_correctness#,\n",
    "              #context_recall,\n",
    "              #context_precision\n",
    "              #context_relevancy\n",
    "          ],\n",
    "        llm=generator_llm,\n",
    "        embeddings=ollama_emb,\n",
    "        raise_exceptions=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pACiLJ9A_nRu",
    "tags": []
   },
   "source": [
    "## PROCESSO AUTOMATIZZATO CON PIU' LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#SE GIA HO CALCOLATO IL TESTSET\n",
    "questions = testset.to_pandas()[\"question\"].to_list()\n",
    "ground_truth = testset.to_pandas()[\"ground_truth\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ALTRIMENTI LO CARICO DAL CSV CHE HO PRESENTE E POI PROCEDO AL CALCOLO\n",
    "import pandas as pd\n",
    "\n",
    "# Carica il CSV in un nuovo DataFrame\n",
    "loaded_df = pd.read_csv('test_dataChatGPT4O_FINALE.csv')\n",
    "\n",
    "# Estrai le colonne 'question' e 'ground_truth'\n",
    "questions = loaded_df[\"question\"].to_list()\n",
    "ground_truth = loaded_df[\"ground_truth\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOBTuCuS_nRu",
    "outputId": "0c4ba4c3-8ef0-4b3b-a766-6b82e65a0155",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, Sequence, Value\n",
    "from tqdm import tqdm\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "# Lista dei modelli da processare\n",
    "model_names = [\"llama3.1\", \"qordmlwls/llama3.1-medical\", \"qwen2\", \"echelonify/med-qwen2\", \"mistral-nemo\", \"phi3\", \"gemma2\"]\n",
    "\n",
    "# Assicurati che questions e ground_truth siano definiti\n",
    "# questions = testset.to_pandas()[\"question\"].to_list()\n",
    "# ground_truth = testset.to_pandas()[\"ground_truth\"].to_list()\n",
    "\n",
    "# Dizionario per memorizzare i risultati di ogni modello\n",
    "resultsDict = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"Processing with model: {model_name}\")\n",
    "    # Inizializza l'LLM per il modello corrente\n",
    "    llm = Ollama(model=model_name)\n",
    "    \n",
    "    # Inizializzazione del dizionario per il modello corrente\n",
    "    data = {\n",
    "        \"question\": [],\n",
    "        \"answer\": [],\n",
    "        \"contexts\": [],\n",
    "        \"ground_truth\": ground_truth\n",
    "    }\n",
    "    \n",
    "    # Utilizzo di tqdm per creare una barra di avanzamento\n",
    "    for query in tqdm(questions, desc=f\"Processing queries for {model_name}\"):\n",
    "        data[\"question\"].append(query)\n",
    "        data[\"answer\"].append(rag_chain.invoke(query))\n",
    "        data[\"contexts\"].append([\"\"])  # Contesto vuoto per ogni elemento\n",
    "    \n",
    "    # Convertire il dizionario in Dataset\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    \n",
    "    # Convertire in DataFrame e memorizzare i risultati\n",
    "    resultsDict[f\"dataset_df_{model_name}\"] = dataset.to_pandas()\n",
    "\n",
    "# Esempio di come accedere ai risultati:\n",
    "# dataset_df_llama3_1 = resultsDict[\"dataset_df_llama3.1\"]\n",
    "# dataset_df_qwen2 = resultsDict[\"dataset_df_qwen2\"]\n",
    "# dataset_df_mistral_nemo = resultsDict[\"dataset_df_mistral-nemo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Salva resultsDict su un file per poter riaccedere successivamente\n",
    "with open('resultsDict_NORAG.pkl', 'wb') as f: #Cambianome per salvare più modelli \n",
    "    pickle.dump(resultsDict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per accedere al singolo dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resultsDict[f\"dataset_df_phi3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X370PdK7_nRv",
    "tags": []
   },
   "source": [
    "AUTOMATIZZAZIONE DELLA VALUTAZIONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "dfd4accfc65e448a91d7f09b6ad3f209",
      "c8a558de1c1646f88b7210bb56ad715d",
      "49f23a1c7f05400697020983fb8a8a0e",
      "0dfa94a7c7b541b2b7102e5adfe970f8",
      "276379ea16ab4c4799e0974299c2bc36",
      "dd6c1e026f244404912ed615fbad98df"
     ]
    },
    "id": "2HoUy39t_nRv",
    "outputId": "b536e7e9-c82f-420f-ddc2-3142dbdbd5e8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from ragas import evaluate\n",
    "from langchain.llms import Ollama\n",
    "from datasets import Dataset\n",
    "\n",
    "# Dizionario per memorizzare i risultati della valutazione\n",
    "evaluation_results = {}\n",
    "\n",
    "# Metriche da utilizzare per la valutazione\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_correctness\n",
    "\n",
    "]\n",
    "\n",
    "# Funzione per convertire DataFrame in Dataset\n",
    "def convert_to_dataset(df):\n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "# Itera su tutti i modelli presenti nel dizionario results\n",
    "for model_name in [\"llama3.1\",\"qordmlwls/llama3.1-medical\",\"qwen2\",\"echelonify/med-qwen2\",\"mistral-nemo\",\"phi3\",\"gemma2\"]: #AGGIUNGERE ALTRI MODELLI...\n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "\n",
    "\n",
    "    # Valutazione per il dataset base\n",
    "    resultBase = evaluate(\n",
    "        dataset=convert_to_dataset(resultsDict[f\"dataset_df_{model_name}\"]),\n",
    "        metrics=metrics,\n",
    "        llm=azure_model_GPT4OMINI, #azure_model_GPT4OMINI o generator_llm  embeddings=azure_embeddings\n",
    "        embeddings=azure_embeddings,\n",
    "        raise_exceptions=False\n",
    "    )\n",
    "\n",
    "    # Memorizza i risultati della valutazione per questo modello\n",
    "    evaluation_results[model_name] = {\n",
    "        \"base\": resultBase\n",
    "    }\n",
    "\n",
    "# Stampa un riepilogo dei risultati\n",
    "for model_name, model_results in evaluation_results.items():\n",
    "    print(f\"\\nResults for {model_name}:\")\n",
    "    for dataset_type, result in model_results.items():\n",
    "        print(f\"  {dataset_type.capitalize()} Dataset:\")\n",
    "        for metric, score in result.items():\n",
    "            print(f\"    {metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywBQW8Oc_nRv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Salva evaluation_results su un file per poter riaccedere successivamente\n",
    "with open('evaluation_results_NORAG.pkl', 'wb') as f: #Cambianome per salvare più modelli \n",
    "    pickle.dump(evaluation_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## CARICO MODELLI PER LE ANALISI DA FILE PICKLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carico il secondo pickle dove ho salvato i risultati degli altri modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install datasets==2.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Carica evaluation_results da un file\n",
    "with open('evaluation_results_NORAG.pkl', 'rb') as f:\n",
    "    evaluation_resultsNORAG = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#LLAMA 3.1\n",
    "LlamaNoRAG = evaluation_resultsNORAG[\"llama3.1\"][\"base\"].to_pandas()\n",
    "\n",
    "# LLAMA 3.1 MED\n",
    "LlamaMedNoRAG = evaluation_resultsNORAG[\"qordmlwls/llama3.1-medical\"][\"base\"].to_pandas()\n",
    "\n",
    "# QWEN2\n",
    "Qwen2NoRAG = evaluation_resultsNORAG[\"qwen2\"][\"base\"].to_pandas()\n",
    "\n",
    "#QWEN MED\n",
    "QwenMedNoRAG = evaluation_resultsNORAG[\"echelonify/med-qwen2\"][\"base\"].to_pandas()\n",
    "\n",
    "#MISTRAL-NEMO\n",
    "MistralNemoNoRAG = evaluation_resultsNORAG[\"mistral-nemo\"][\"base\"].to_pandas()\n",
    "\n",
    "#PHI3\n",
    "Phi3NoRAG = evaluation_resultsNORAG[\"phi3\"][\"base\"].to_pandas()\n",
    "\n",
    "#GEMMA2\n",
    "Gemma2NoRAG = evaluation_resultsNORAG[\"gemma2\"][\"base\"].to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LlamaNoRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Carico modello per analizzare i modelli basati su RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Carica evaluation_results da un file\n",
    "with open('evaluation_results_MODELLICOMP.pkl', 'rb') as f:\n",
    "    evaluation_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#LLAMA 3.1\n",
    "LlamaBase = evaluation_results[\"llama3.1\"][\"base\"].to_pandas()\n",
    "LlamaMulti = evaluation_results[\"llama3.1\"][\"multi\"].to_pandas()\n",
    "LlamaEnsemble = evaluation_results[\"llama3.1\"][\"ensemble\"].to_pandas()\n",
    "\n",
    "# LLAMA 3.1 MED\n",
    "LlamaMedBase = evaluation_results[\"qordmlwls/llama3.1-medical\"][\"base\"].to_pandas()\n",
    "LlamaMedMulti = evaluation_results[\"qordmlwls/llama3.1-medical\"][\"multi\"].to_pandas()\n",
    "LlamaMedEnsemble = evaluation_results[\"qordmlwls/llama3.1-medical\"][\"ensemble\"].to_pandas()\n",
    "\n",
    "# QWEN2\n",
    "Qwen2Base = evaluation_results[\"qwen2\"][\"base\"].to_pandas()\n",
    "Qwen2Multi = evaluation_results[\"qwen2\"][\"multi\"].to_pandas()\n",
    "Qwen2Ensemble = evaluation_results[\"qwen2\"][\"ensemble\"].to_pandas()\n",
    "\n",
    "#QWEN MED\n",
    "QwenMedBase = evaluation_results[\"echelonify/med-qwen2\"][\"base\"].to_pandas()\n",
    "QwenMedMulti = evaluation_results[\"echelonify/med-qwen2\"][\"multi\"].to_pandas()\n",
    "QwenMedEnsemble = evaluation_results[\"echelonify/med-qwen2\"][\"ensemble\"].to_pandas()\n",
    "\n",
    "#MISTRAL-NEMO\n",
    "MistralNemoBase = evaluation_results[\"mistral-nemo\"][\"base\"].to_pandas()\n",
    "MistralNemoMulti = evaluation_results[\"mistral-nemo\"][\"multi\"].to_pandas()\n",
    "MistralNemoEnsemble = evaluation_results[\"mistral-nemo\"][\"ensemble\"].to_pandas()\n",
    "\n",
    "#PHI3\n",
    "Phi3Base = evaluation_results[\"phi3\"][\"base\"].to_pandas()\n",
    "Phi3Multi = evaluation_results[\"phi3\"][\"multi\"].to_pandas()\n",
    "Phi3Ensemble = evaluation_results[\"phi3\"][\"ensemble\"].to_pandas()\n",
    "\n",
    "#GEMMA2\n",
    "Gemma2Base = evaluation_results[\"gemma2\"][\"base\"].to_pandas()\n",
    "Gemma2Multi = evaluation_results[\"gemma2\"][\"multi\"].to_pandas()\n",
    "Gemma2Ensemble = evaluation_results[\"gemma2\"][\"ensemble\"].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZHdQXAK_nRw",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ANALISI RISULTATI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WR3J3UtP_nRw"
   },
   "source": [
    "CONTROLLO I VARI RISULTATI OTTENUTI PER SINGOLO RETRIEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tecdgtf4_nRw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "Phi3BaseSingolo = evaluation_results[\"phi3\"][\"base\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Phi3BaseSingolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluation_resultsNEW = evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluation_resultsNEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "puglvNFO_nRw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#resultMulti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRo-Pzsf_nRw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#resultEnsemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wD2H7M4_nRx"
   },
   "source": [
    "PER CONTROLLARE SUL DIZIONARIO OTTENUTO DAL PROCESSO AUTOMATIZZATO ACCEDO PRIMA AL SINGOLO DATASET DEL LLM IN QUESTIONE PER VERIFICARE E POI ESEGUO ALTRI TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Carica ResultDict da un file\n",
    "with open('resultsDict_4MODELLI.pkl', 'rb') as f:\n",
    "    resultsDict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NF243rfw_nRx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#SE VOGLIO TESTARE SU SINGOLO DATASET ALTRE METRICHE MI PRENDO IL DATASET DAL DIZIONARIO(SE NON E' CREATO, LO CARICO)- IN QUESTO CASO PER ESEMPIO E' PHI3 MA PER ALTRI BASTA MODIFICARE IL NOME\n",
    "DataBasePhi3= convert_to_dataset(resultsDict[\"dataset_df_phi3\"])\n",
    "DataMultiPhi3= convert_to_dataset(resultsDict[\"advanced_retrieval_df_phi3\"])\n",
    "DataEnsemblePhi3= convert_to_dataset(resultsDict[\"ensemble_retrieval_df_phi3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### PER AGGIUNGERE NUOVA METRICA AL DIZIONARIO PRECEDENTE(MANUALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_metric_score = evaluate(DataBasePhi3, metrics=[harmfulness], llm=azure_model_GPT4OMINI, embeddings=azure_embeddings)\n",
    "new_metric_df = new_metric_score.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assumendo che la nuova metrica sia una singola colonna\n",
    "new_metric_name = \"harmfulness\"  # Sostituisci con il nome effettivo della metrica\n",
    "Phi3Base[new_metric_name] = new_metric_df[new_metric_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Phi3Base[new_metric_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluation_results[\"phi3\"][\"base\"] = Dataset.from_pandas(Phi3Base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Phi3Base = evaluation_results[\"phi3\"][\"base\"].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PER ELIMINARE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Supponiamo che la colonna che vuoi eliminare si chiami \"harmfulness\"\n",
    "column_to_remove = \"harmfulness\"\n",
    "\n",
    "# Rimuovi la colonna da Phi3Base\n",
    "Phi3Base = Phi3Base.drop(columns=[column_to_remove])\n",
    "\n",
    "# Aggiorna il dizionario evaluation_results\n",
    "evaluation_results[\"phi3\"][\"base\"] = Dataset.from_pandas(Phi3Base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USARE PER SOVRASCRIVERE IL FILE PICKLE PRECEDENTE AGGIORNANDOLO\n",
    "with open('evaluation_results_4MODELLI_UPDATE.pkl', 'wb') as f:\n",
    "    pickle.dump(evaluation_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "daae9214eb9a4708ab8f09e1911b3b2b"
     ]
    },
    "id": "gB6Vfbpy_nRx",
    "outputId": "9ea4f15c-1279-45fe-e448-c7a06a6e4dae",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#PER TESTARE SINGOLA METRICA\n",
    "#score = evaluate(DataBasePhi3,metrics=[harmfulness], llm=azure_model_GPT4OMINI, embeddings=azure_embeddings)\n",
    "#score.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### AGGIUNTA DI METRICHE AUTOMATIZZATO PER TUTTI I MODELLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def add_new_metric_to_all_models(resultsDict, evaluation_results, metric_function, metric_name, llm, embeddings):\n",
    "    models = [\"qordmlwls/llama3.1-medical\", \"qwen2\", \"mistral-nemo\", \"phi3\"]\n",
    "    approaches = {\n",
    "        \"base\": \"dataset_df_\",\n",
    "        \"multi\": \"advanced_retrieval_df_\",\n",
    "        \"ensemble\": \"ensemble_retrieval_df_\"\n",
    "    }\n",
    "    \n",
    "    total_iterations = len(models) * len(approaches)\n",
    "    progress_bar = tqdm(total=total_iterations, desc=\"Processing models\")\n",
    "\n",
    "    for model in models:\n",
    "        for approach, prefix in approaches.items():\n",
    "            progress_bar.set_description(f\"Processing {model} - {approach}\")\n",
    "            \n",
    "            # Ottieni il dataset da resultsDict\n",
    "            dataset_key = f\"{prefix}{model}\"\n",
    "            current_dataset = convert_to_dataset(resultsDict[dataset_key])\n",
    "            \n",
    "            # Calcola la nuova metrica\n",
    "            new_metric_score = evaluate(current_dataset, metrics=[metric_function], llm=llm, embeddings=embeddings)\n",
    "            new_metric_df = new_metric_score.to_pandas()\n",
    "            \n",
    "            # Aggiunge la nuova metrica al DataFrame esistente in evaluation_results\n",
    "            current_df = evaluation_results[model][approach].to_pandas()\n",
    "            current_df[metric_name] = new_metric_df[metric_name]\n",
    "            \n",
    "            # Aggiorna il dizionario evaluation_results\n",
    "            evaluation_results[model][approach] = Dataset.from_pandas(current_df)\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "    \n",
    "    progress_bar.close()\n",
    "    return evaluation_results\n",
    "\n",
    "# Uso della funzione\n",
    "new_evaluation_results = add_new_metric_to_all_models(\n",
    "    resultsDict,\n",
    "    evaluation_results, \n",
    "    harmfulness,  # La funzione metrica che hai definito\n",
    "    \"harmfulness\",  # Il nome della nuova metrica\n",
    "    azure_model_GPT4OMINI, \n",
    "    azure_embeddings\n",
    ")\n",
    "\n",
    "# Salva il dizionario aggiornato nel file pickle\n",
    "with open('evaluation_results_4MODELLI_UPDATE.pkl', 'wb') as f:\n",
    "    pickle.dump(new_evaluation_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_evaluation_results[\"qordmlwls/llama3.1-medical\"][\"base\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## CALCOLO MEDIA DELLE METRICHE\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics_mean(dataset):\n",
    "    metrics = ['faithfulness', 'answer_relevancy', 'answer_correctness', 'context_recall', 'context_precision', 'harmfulness']\n",
    "    \n",
    "    means = {}\n",
    "    for metric in metrics:\n",
    "        values = dataset[metric]\n",
    "        mean_value = np.mean(values)\n",
    "        means[metric] = mean_value\n",
    "    \n",
    "    return means\n",
    "\n",
    "# Uso della funzione\n",
    "model = \"qordmlwls/llama3.1-medical\"\n",
    "approach = \"base\"\n",
    "dataset = new_evaluation_results[model][approach]\n",
    "\n",
    "metric_means = calculate_metrics_mean(dataset)\n",
    "\n",
    "# Stampa i risultati\n",
    "for metric, mean in metric_means.items():\n",
    "    print(f\"{metric}: {mean:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Carico per testare\n",
    "with open('evaluation_results_4MODELLI_UPDATE.pkl', 'rb') as f:\n",
    "    evaluation_update = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluation_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLAMA 3.1 MED\n",
    "LlamaMedBaseU = evaluation_update[\"qordmlwls/llama3.1-medical\"][\"base\"].to_pandas()\n",
    "LlamaMedMultiU = evaluation_update[\"qordmlwls/llama3.1-medical\"][\"multi\"].to_pandas()\n",
    "LlamaMedEnsembleU = evaluation_update[\"qordmlwls/llama3.1-medical\"][\"ensemble\"].to_pandas()\n",
    "\n",
    "# QWEN2\n",
    "Qwen2BaseU = evaluation_update[\"qwen2\"][\"base\"].to_pandas()\n",
    "Qwen2MultiU = evaluation_update[\"qwen2\"][\"multi\"].to_pandas()\n",
    "Qwen2EnsembleU = evaluation_update[\"qwen2\"][\"ensemble\"].to_pandas()\n",
    "\n",
    "#MISTRAL-NEMO\n",
    "MistralNemoBaseU = evaluation_update[\"mistral-nemo\"][\"base\"].to_pandas()\n",
    "MistralNemoMultiU = evaluation_update[\"mistral-nemo\"][\"multi\"].to_pandas()\n",
    "MistralNemoEnsembleU = evaluation_update[\"mistral-nemo\"][\"ensemble\"].to_pandas()\n",
    "\n",
    "#PHI3\n",
    "Phi3BaseU = evaluation_update[\"phi3\"][\"base\"].to_pandas()\n",
    "Phi3MultiU = evaluation_update[\"phi3\"][\"multi\"].to_pandas()\n",
    "Phi3EnsembleU = evaluation_update[\"phi3\"][\"ensemble\"].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5ZZkFh6_nRx",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## CREO IL DATASET DI CONFRONTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluation_results #RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_resultsNORAG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Funzione per creare un DataFrame di confronto per un singolo modello\n",
    "def create_comparison_df(base_results, noRAG_results, model_name):\n",
    "    # Creazione del DataFrame per BaseRetriever\n",
    "    df_base = pd.DataFrame(list(base_results['base'].items()), columns=['Metric', 'BaseRetriever'])\n",
    "    \n",
    "    # Creazione del DataFrame per NoRAG retriever\n",
    "    df_noRAG = pd.DataFrame(list(noRAG_results['base'].items()), columns=['Metric', 'NoRAGRetriever'])\n",
    "\n",
    "    # Unire i due DataFrame basandosi sulla metrica\n",
    "    df_merged = pd.merge(df_base, df_noRAG, on='Metric')\n",
    "\n",
    "    # Aggiungere una colonna con il nome del modello\n",
    "    df_merged['Model'] = model_name\n",
    "\n",
    "    return df_merged\n",
    "\n",
    "# Creare un DataFrame di confronto per ogni modello\n",
    "comparison_dfs = {}\n",
    "for model_name, base_results in evaluation_results.items():\n",
    "    # Estrazione dei risultati corrispondenti dai dati NoRAG\n",
    "    noRAG_results = evaluation_resultsNORAG[model_name]\n",
    "\n",
    "    # Creazione del DataFrame di confronto\n",
    "    comparison_dfs[model_name] = create_comparison_df(base_results, noRAG_results, model_name)\n",
    "\n",
    "# Combinare tutti i DataFrame in un unico DataFrame di confronto\n",
    "combined_df = pd.DataFrame()\n",
    "for df in comparison_dfs.values():\n",
    "    if combined_df.empty:\n",
    "        combined_df = df\n",
    "    else:\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "# Riorganizzare le colonne per avere 'Model' all'inizio, seguito dalle metriche\n",
    "column_order = ['Model', 'Metric', 'BaseRetriever', 'NoRAGRetriever']\n",
    "combined_df = combined_df[column_order]\n",
    "\n",
    "# Visualizzare il DataFrame combinato\n",
    "#print(combined_df)\n",
    "\n",
    "# Opzionalmente, salvare il DataFrame in un file CSV\n",
    "combined_df.to_csv('llm_rag_noRAG.csv', index=False)\n",
    "\n",
    "# Visualizzazione del DataFrame\n",
    "combined_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJRqMXsB_nR0",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## CREO LA MATRICE DI CONFUSIONE PER VALUTARE BENE IL MIO RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sS7kKES__nR0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "#df = result.to_pandas()\n",
    "\n",
    "heatmap_data = QwenMedEnsemble[['faithfulness', 'answer_relevancy', 'answer_correctness']]\n",
    "\n",
    "cmap = LinearSegmentedColormap.from_list('green_red', ['red', 'green'])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".2f\", linewidths=.5, cmap=cmap)\n",
    "\n",
    "plt.yticks(ticks=range(len(LlamaMedNoRAG['question'])), labels=LlamaMedNoRAG['question'], rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acU3JIGj_nR1",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### VALUTAZIONE VARI RETRIEVER CON GRAFICO A RADAR E BARRE PER SINGOLO MODELLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ez5D8ZCO_nR1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MI ESTRAGGO I RISULTATI SINGOLARMENTTE\n",
    "llamaBase = evaluation_results[\"llama3.1\"][\"base\"].to_pandas()\n",
    "llamaMulti = evaluation_results[\"llama3.1\"][\"multi\"].to_pandas()\n",
    "llamaEnsemble = evaluation_results[\"llama3.1\"][\"ensemble\"].to_pandas()\n",
    "#FARE PER ALTRI MODELLI..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "QwenMedBase = evaluation_results[\"echelonify/med-qwen2\"][\"base\"].to_pandas()\n",
    "QwenMedMulti = evaluation_results[\"echelonify/med-qwen2\"][\"multi\"].to_pandas()\n",
    "QwenMedEnsemble = evaluation_results[\"echelonify/med-qwen2\"][\"ensemble\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fG-yMju_nR1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "QwenMedBase[metrics] # VERIFICO LE METRICHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "metrics = ['answer_relevancy', 'faithfulness', 'answer_correctness']\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for data, name, color in zip([Qwen2Base, Qwen2Multi, Qwen2Ensemble],\n",
    "                             ['Qwen2 Base', 'Qwen2 Multi', 'Qwen2 Ensemble'],\n",
    "                             ['blue', 'red', 'green']):\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=data[metrics].values.flatten(),\n",
    "        theta=metrics,\n",
    "        fill='toself',\n",
    "        name=name,\n",
    "        line=dict(color=color, width=2),\n",
    "        marker=dict(size=8, symbol='circle')\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(\n",
    "        radialaxis=dict(\n",
    "            visible=True,\n",
    "            range=[0, 1],\n",
    "            tickvals=[0.2, 0.4, 0.6, 0.8],\n",
    "            ticktext=['20%', '40%', '60%', '80%']\n",
    "        ),\n",
    "        angularaxis=dict(\n",
    "            tickfont=dict(size=12)\n",
    "        )\n",
    "    ),\n",
    "    showlegend=True,\n",
    "    legend=dict(font=dict(size=12)),\n",
    "    title=dict(\n",
    "        text=\"Retrieval Augmented Generation - Evaluation for Qwen2\",\n",
    "        font=dict(size=16)\n",
    "    ),\n",
    "    width=700,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "metrics = ['answer_relevancy', 'faithfulness', 'answer_correctness']\n",
    "models = ['QwenMed Base', 'QwenMed Multi', 'QwenMed Ensemble']\n",
    "colors = ['blue', 'red', 'green']\n",
    "\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=metrics)\n",
    "\n",
    "for i, metric in enumerate(metrics, start=1):\n",
    "    for j, (data, name, color) in enumerate(zip([QwenMedBase, QwenMedMulti, QwenMedEnsemble], models, colors)):\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=[name], \n",
    "                y=[data[metric].values[0]], \n",
    "                name=name,\n",
    "                marker_color=color,\n",
    "                showlegend=i==1\n",
    "            ),\n",
    "            row=1, col=i\n",
    "        )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(\n",
    "        text=\"Retrieval Augmented Generation - Evaluation for QwenMed 2\",\n",
    "        font=dict(size=16)\n",
    "    ),\n",
    "    barmode='group',\n",
    "    height=500,\n",
    "    width=900,\n",
    "    yaxis=dict(range=[0, 1], tickformat='.0%'),\n",
    "    yaxis2=dict(range=[0, 1], tickformat='.0%'),\n",
    "    yaxis3=dict(range=[0, 1], tickformat='.0%')\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "metrics = ['answer_relevancy', 'faithfulness', 'answer_correctness']\n",
    "models = ['QwenMed Base', 'QwenMed Multi', 'QwenMed Ensemble']\n",
    "colors = ['blue', 'red', 'green']\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for data, name, color in zip([QwenMedBase, QwenMedMulti, QwenMedEnsemble], models, colors):\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=name,\n",
    "        x=metrics,\n",
    "        y=data[metrics].values.flatten(),\n",
    "        marker_color=color\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(\n",
    "        text=\"Retrieval Augmented Generation - Evaluation for QwenMed 2\",\n",
    "        font=dict(size=16)\n",
    "    ),\n",
    "    barmode='group',\n",
    "    yaxis=dict(range=[0, 1], tickformat='.0%'),\n",
    "    height=500,\n",
    "    width=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPG09Ld7_nRy",
    "tags": []
   },
   "source": [
    "## GRAFICI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPCszcFY_nRy",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### GRAFICO DI COMPARAZIONE TRA DUE MODELLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUBJ4wIV_nRz",
    "outputId": "48290260-1d2c-47b3-ec07-5132c8bb78b4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Funzione di analisi\n",
    "def analysis(model1, model2):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(12, 5))\n",
    "    columns = model1.columns\n",
    "    for i, col in enumerate(columns):\n",
    "        sns.kdeplot(data=model1[col].dropna(), legend=False, ax=axs[i], fill=True, label='Qwen2') #MODIFICA NOME MODELLO\n",
    "        sns.kdeplot(data=model2[col].dropna(), legend=False, ax=axs[i], fill=True, label='Qwen2 Med') #MODIFICA NOME MODELLO\n",
    "        axs[i].set_title(f'{col} scores distribution')\n",
    "        axs[i].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Chiamata alla funzione di analisi\n",
    "analysis(\n",
    "    Qwen2Base[['faithfulness', 'answer_relevancy', 'answer_correctness']],\n",
    "    QwenMedBase[['faithfulness', 'answer_relevancy', 'answer_correctness']]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### GRAFICO COMPARAZIONE PIU' MODELLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def multi_model_analysis(*args):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    metrics = ['faithfulness', 'answer_relevancy', 'answer_correctness']\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        for df, label in args:\n",
    "            sns.kdeplot(data=df[metric].dropna(), ax=axs[i], fill=True, label=label)\n",
    "        \n",
    "        axs[i].set_title(f'{metric} scores distribution')\n",
    "        axs[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Chiamata alla funzione di analisi\n",
    "multi_model_analysis(\n",
    "    (LlamaBase[metrics], 'Llama-Base'),\n",
    "   # (LlamaMulti[metrics], 'Llama-Multi'),\n",
    "   # (LlamaEnsemble[metrics], 'Llama-Ensemble'),\n",
    "    (LlamaMedBase[metrics], 'LlamaMed-Base'),\n",
    "   # (LlamaMedMulti[metrics], 'LlamaMed-Multi'),\n",
    "   # (LlamaMedEnsemble[metrics], 'LlamaMed-Ensemble'),\n",
    "    (Qwen2Base[metrics], 'Qwen2-Base'),\n",
    "   # (Qwen2Multi[metrics], 'Qwen2-Multi'),\n",
    "   # (Qwen2Ensemble[metrics], 'Qwen2-Ensemble'),\n",
    "    (QwenMedBase[metrics], 'QwenMed-Base'),\n",
    "   # (QwenMedMulti[metrics], 'QwenMed-Multi'),\n",
    "   # (QwenMedEnsemble[metrics], 'QwenMed-Ensemble'),\n",
    "    (MistralNemoBase[metrics], 'MistralNemo-Base'),\n",
    "   #(MistralNemoMulti[metrics], 'MistralNemo-Multi'),\n",
    "   # (MistralNemoEnsemble[metrics], 'MistralNemo-Ensemble'),\n",
    "    (Phi3Base[metrics], 'Phi3-Base'),\n",
    "   # (Phi3Multi[metrics], 'Phi3-Multi'),\n",
    "   # (Phi3Ensemble[metrics], 'Phi3-Ensemble')\n",
    "    (Gemma2Base[metrics], 'Gemma2-Base')\n",
    "   # (Gemma2Multi[metrics], 'Gemma2-Multi'),\n",
    "   # (Gemma2Ensemble[metrics], 'Gemma2-Ensemble')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ALTRI GRAFICI DA PROVATI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### HEATMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Preparazione dei dati\n",
    "models = ['Llama','LlamaMed', 'Qwen2','QwenMed', 'MistralNemo', 'Phi3','Gemma2'] #Modelli da testare : 'Qwen2','QwenMed', 'MistralNemo', 'Phi3','Gemma2'\n",
    "retrievers = ['Base','NoRAG' ]#'Multi', 'Ensemble'\n",
    "metrics = ['faithfulness', 'answer_relevancy', 'answer_correctness']\n",
    "\n",
    "data = []\n",
    "for model in models:\n",
    "    for retriever in retrievers:\n",
    "        model_data = globals()[f\"{model}{retriever}\"]\n",
    "        avg_scores = model_data[metrics].mean()\n",
    "        data.append([f\"{model}-{retriever}\"] + list(avg_scores))\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Model'] + metrics)\n",
    "df_pivot = df.set_index('Model').T\n",
    "\n",
    "# Creazione della heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df_pivot, annot=True, cmap='YlGnBu', fmt='.3f')\n",
    "plt.title('Model Comparison - Average Scores')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### RADAR CHART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def radar_chart(data, categories, title):\n",
    "    N = len(categories)\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "    plt.xticks(angles[:-1], categories)\n",
    "\n",
    "    for i, (model, scores) in enumerate(data.items()):\n",
    "        values = scores + scores[:1]\n",
    "        ax.plot(angles, values, linewidth=1, linestyle='solid', label=model)\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "\n",
    "# Preparazione dei dati\n",
    "models = ['LlamaMed']#'Qwen2','QwenMed', 'MistralNemo', 'Phi3'\n",
    "retrievers = ['Base']#'Multi', 'Ensemble\n",
    "metrics = ['faithfulness', 'answer_relevancy', 'answer_correctness']\n",
    "\n",
    "data = {}\n",
    "for model in models:\n",
    "    for retriever in retrievers:\n",
    "        model_data = globals()[f\"{model}{retriever}\"]\n",
    "        avg_scores = model_data[metrics].mean()\n",
    "        data[f\"{model}-{retriever}\"] = list(avg_scores)\n",
    "\n",
    "# Creazione del radar chart\n",
    "plt.figure(figsize=(10, 10))\n",
    "radar_chart(data, metrics, 'Model Comparison - Average Scores')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### BAR PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Preparazione dei dati\n",
    "models = ['Llama','LlamaMed']#'Qwen2','QwenMed', 'MistralNemo', 'Phi3','Gemma2'\n",
    "retrievers = ['Base', 'Multi', 'Ensemble']\n",
    "metrics = ['faithfulness', 'answer_relevancy','answer_correctness' ]#'answer_relevancy''faithfulness',\n",
    "\n",
    "data = []\n",
    "for model in models:\n",
    "    for retriever in retrievers:\n",
    "        model_data = globals()[f\"{model}{retriever}\"]\n",
    "        avg_scores = model_data[metrics].mean()\n",
    "        data.append([f\"{model}-{retriever}\"] + list(avg_scores))\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Model'] + metrics)\n",
    "\n",
    "# Creazione del bar plot\n",
    "df.set_index('Model').plot(kind='bar', figsize=(15, 8))\n",
    "plt.title('Model Comparison - Average Scores')\n",
    "plt.xlabel('Model-Retriever')\n",
    "plt.ylabel('Average Score')\n",
    "plt.legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### CON ETICHETTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Preparazione dei dati\n",
    "models = ['Llama', 'LlamaMed']  # 'Qwen2','QwenMed', 'MistralNemo', 'Phi3','Gemma2'\n",
    "retrievers = ['Base', 'Multi', 'Ensemble']\n",
    "metrics = ['faithfulness', 'answer_relevancy', 'answer_correctness']\n",
    "data = []\n",
    "for model in models:\n",
    "    for retriever in retrievers:\n",
    "        model_data = globals()[f\"{model}{retriever}\"]\n",
    "        avg_scores = model_data[metrics].mean()\n",
    "        data.append([f\"{model}-{retriever}\"] + list(avg_scores))\n",
    "df = pd.DataFrame(data, columns=['Model'] + metrics)\n",
    "\n",
    "# Creazione del bar plot\n",
    "ax = df.set_index('Model').plot(kind='bar', figsize=(15, 8))\n",
    "plt.title('Model Comparison - Average Scores')\n",
    "plt.xlabel('Model-Retriever')\n",
    "plt.ylabel('Average Score')\n",
    "plt.legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Aggiunta delle etichette sopra ogni barra\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.3f', padding=3)\n",
    "\n",
    "# Aggiustamento del layout per evitare sovrapposizioni\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### RAG_BASERETRIEVER VS NORAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Preparazione dei dati\n",
    "models = ['Llama', 'LlamaMed', 'Qwen2', 'QwenMed', 'MistralNemo', 'Phi3', 'Gemma2']\n",
    "retrievers = ['Base', 'NoRAG']\n",
    "metrics = ['faithfulness', 'answer_relevancy', 'answer_correctness']\n",
    "\n",
    "data = []\n",
    "for model in models:\n",
    "    for retriever in retrievers:\n",
    "        model_data = globals()[f\"{model}{retriever}\"]\n",
    "        avg_scores = model_data[metrics].mean()\n",
    "        data.append([model, retriever] + list(avg_scores))\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Model', 'Retriever'] + metrics)\n",
    "\n",
    "# Creazione del grafico a barre raggruppate\n",
    "plt.figure(figsize=(14, 8))\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "base_scores = df[df['Retriever'] == 'Base']['answer_correctness'].values\n",
    "norag_scores = df[df['Retriever'] == 'NoRAG']['answer_correctness'].values\n",
    "\n",
    "plt.bar(x - width/2, base_scores, width, label='RAG', color='skyblue')\n",
    "plt.bar(x + width/2, norag_scores, width, label='NoRAG', color='lightgreen')\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Answer Correctness Score')\n",
    "plt.title('Comparison of RAG vs NoRAG - Answer Correctness')\n",
    "plt.xticks(x, models, rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "# Aggiungi i valori sopra le barre\n",
    "for i, v in enumerate(base_scores):\n",
    "    plt.text(i - width/2, v, f'{v:.3f}', ha='center', va='bottom')\n",
    "for i, v in enumerate(norag_scores):\n",
    "    plt.text(i + width/2, v, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo i tre grafici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Creazione della cartella immaginiPDF se non esiste\n",
    "if not os.path.exists('immaginiPDF'):\n",
    "    os.makedirs('immaginiPDF')\n",
    "\n",
    "# Preparazione dei dati\n",
    "models = ['Llama', 'LlamaMed', 'Qwen2', 'QwenMed', 'MistralNemo', 'Phi3', 'Gemma2']\n",
    "retrievers = ['Base', 'NoRAG']\n",
    "metrics = ['faithfulness', 'answer_relevancy', 'answer_correctness']\n",
    "data = []\n",
    "for model in models:\n",
    "    for retriever in retrievers:\n",
    "        model_data = globals()[f\"{model}{retriever}\"]\n",
    "        avg_scores = model_data[metrics].mean()\n",
    "        data.append([model, retriever] + list(avg_scores))\n",
    "df = pd.DataFrame(data, columns=['Model', 'Retriever'] + metrics)\n",
    "\n",
    "# Funzione per creare il grafico\n",
    "def create_plot(metric):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.rcParams['font.size'] = 10\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    base_scores = df[df['Retriever'] == 'Base'][metric].values\n",
    "    norag_scores = df[df['Retriever'] == 'NoRAG'][metric].values\n",
    "    plt.bar(x - width/2, base_scores, width, label='RAG', color='skyblue')\n",
    "    plt.bar(x + width/2, norag_scores, width, label='NoRAG', color='lightgreen')\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel(f'{metric.capitalize()} Score')\n",
    "    plt.title(f'Comparison of RAG vs NoRAG - {metric.capitalize()}')\n",
    "    plt.xticks(x, models, rotation=45)\n",
    "    plt.legend()\n",
    "    # Aggiungi i valori sopra le barre\n",
    "    for i, v in enumerate(base_scores):\n",
    "        plt.text(i - width/2, v, f'{v:.3f}', ha='center', va='bottom')\n",
    "    for i, v in enumerate(norag_scores):\n",
    "        plt.text(i + width/2, v, f'{v:.3f}', ha='center', va='bottom')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Salva il grafico in PDF\n",
    "    pdf_path = os.path.join('immaginiPDF', f'rag_vs_norag_{metric}.pdf')\n",
    "    plt.savefig(pdf_path, format='pdf', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Grafico salvato come {pdf_path}\")\n",
    "    \n",
    "    # Visualizza il grafico\n",
    "    plt.show()\n",
    "\n",
    "# Crea un grafico per ciascuna metrica\n",
    "for metric in metrics:\n",
    "    create_plot(metric)\n",
    "\n",
    "print(\"I grafici sono stati salvati nella cartella 'immaginiPDF' come 'rag_vs_norag_faithfulness.pdf', 'rag_vs_norag_answer_relevancy.pdf', e 'rag_vs_norag_answer_correctness.pdf'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### RAG_MULTIRETRIEVER VS NORAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Preparazione dei dati\n",
    "models = ['Llama', 'LlamaMed', 'Qwen2', 'QwenMed', 'MistralNemo', 'Phi3', 'Gemma2']\n",
    "retrievers = ['Multi', 'NoRAG']\n",
    "metrics = ['faithfulness', 'answer_relevancy', 'answer_correctness']\n",
    "\n",
    "data = []\n",
    "for model in models:\n",
    "    for retriever in retrievers:\n",
    "        model_data = globals()[f\"{model}{retriever}\"]\n",
    "        avg_scores = model_data[metrics].mean()\n",
    "        data.append([model, retriever] + list(avg_scores))\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Model', 'Retriever'] + metrics)\n",
    "\n",
    "# Funzione per creare il grafico\n",
    "def create_plot(metric):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.rcParams['font.size'] = 10\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "\n",
    "    multi_scores = df[df['Retriever'] == 'Multi'][metric].values\n",
    "    norag_scores = df[df['Retriever'] == 'NoRAG'][metric].values\n",
    "\n",
    "    plt.bar(x - width/2, multi_scores, width, label='RAG Multi', color='skyblue')\n",
    "    plt.bar(x + width/2, norag_scores, width, label='NoRAG', color='lightgreen')\n",
    "\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel(f'{metric.capitalize()} Score')\n",
    "    plt.title(f'Comparison of RAG vs NoRAG - {metric.capitalize()}')\n",
    "    plt.xticks(x, models, rotation=45)\n",
    "    plt.legend()\n",
    "    # Aggiungere spazio per la legenda\n",
    "    plt.subplots_adjust(right=0.75)\n",
    "    # Posizionare la legenda a destra fuori dal grafico\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 1))\n",
    "    # Aggiungi i valori sopra le barre\n",
    "    for i, v in enumerate(multi_scores):\n",
    "        plt.text(i - width/2, v, f'{v:.3f}', ha='center', va='bottom')\n",
    "    for i, v in enumerate(norag_scores):\n",
    "        plt.text(i + width/2, v, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Crea un grafico per ciascuna metrica\n",
    "for metric in metrics:\n",
    "    create_plot(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### RAG_ENSEMBLERETRIEVER VS NORAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Preparazione dei dati\n",
    "models = ['Llama', 'LlamaMed', 'Qwen2', 'QwenMed', 'MistralNemo', 'Phi3', 'Gemma2']\n",
    "retrievers = ['Ensemble', 'NoRAG']\n",
    "metrics = ['faithfulness', 'answer_relevancy', 'answer_correctness']\n",
    "\n",
    "data = []\n",
    "for model in models:\n",
    "    for retriever in retrievers:\n",
    "        model_data = globals()[f\"{model}{retriever}\"]\n",
    "        avg_scores = model_data[metrics].mean()\n",
    "        data.append([model, retriever] + list(avg_scores))\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Model', 'Retriever'] + metrics)\n",
    "\n",
    "# Funzione per creare il grafico\n",
    "def create_plot(metric):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.rcParams['font.size'] = 10\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "\n",
    "    ensemble_scores = df[df['Retriever'] == 'Ensemble'][metric].values\n",
    "    norag_scores = df[df['Retriever'] == 'NoRAG'][metric].values\n",
    "\n",
    "    plt.bar(x - width/2, ensemble_scores, width, label='RAG Ensemble', color='skyblue')\n",
    "    plt.bar(x + width/2, norag_scores, width, label='NoRAG', color='lightgreen')\n",
    "\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel(f'{metric.capitalize()} Score')\n",
    "    plt.title(f'Comparison of RAG vs NoRAG - {metric.capitalize()}')\n",
    "    plt.xticks(x, models, rotation=45)\n",
    "    plt.legend()\n",
    "    # Aggiungere spazio per la legenda\n",
    "    plt.subplots_adjust(right=0.75)\n",
    "    # Posizionare la legenda a destra fuori dal grafico\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 1))\n",
    "    # Aggiungi i valori sopra le barre\n",
    "    for i, v in enumerate(ensemble_scores):\n",
    "        plt.text(i - width/2, v, f'{v:.3f}', ha='center', va='bottom')\n",
    "    for i, v in enumerate(norag_scores):\n",
    "        plt.text(i + width/2, v, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Crea un grafico per ciascuna metrica\n",
    "for metric in metrics:\n",
    "    create_plot(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00dcae1beca8478e8c596ca973954bf3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "08b8dc02bd6342518d6c3ed06aad4196": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f6e7d459ca647109c754c044a13b8ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "11b7ca5400e84648a6e364d2f8cc193c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_85fb31282c494959bb8184607dca6021",
       "IPY_MODEL_e05b9e80147749fcbda2a8b974116880",
       "IPY_MODEL_fb09d567dba94414b09751f20bc78fd3"
      ],
      "layout": "IPY_MODEL_00dcae1beca8478e8c596ca973954bf3"
     }
    },
    "1d4f00f9d07e4be6a9e0fa84432640c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b586dc188434998a6d47e984d1a7d8d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ba044f88fd64a39bf6867db26aa8c98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4f2478ce413b464b9733fbc8368ae692": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "50bab7802bc147e98528e7d469f8f649": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "50d66d72b8b04452bd8733ec87cdcc01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9cea605ab2524f2983d0b526bf595c6c",
       "IPY_MODEL_be529eb916a04978949d42a5f70e0e28",
       "IPY_MODEL_9c295d4550d04e28a6c502a53c268476"
      ],
      "layout": "IPY_MODEL_f519b956aa1b45a3b552977a7da173eb"
     }
    },
    "5c94413d058249259d98dde145623926": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "84762018def04806aa56a3b002062719": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85fb31282c494959bb8184607dca6021": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_50bab7802bc147e98528e7d469f8f649",
      "placeholder": "​",
      "style": "IPY_MODEL_ebe65f02f66b4c19b7847c01af5e6d0a",
      "value": "embedding nodes: 100%"
     }
    },
    "9c295d4550d04e28a6c502a53c268476": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f2478ce413b464b9733fbc8368ae692",
      "placeholder": "​",
      "style": "IPY_MODEL_5c94413d058249259d98dde145623926",
      "value": " 27/31 [00:59&lt;00:13,  3.31s/it]"
     }
    },
    "9cea605ab2524f2983d0b526bf595c6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d4f00f9d07e4be6a9e0fa84432640c5",
      "placeholder": "​",
      "style": "IPY_MODEL_0f6e7d459ca647109c754c044a13b8ff",
      "value": "Generating:  87%"
     }
    },
    "ab31e4467f5f4e70b536b8de0675ebb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "be529eb916a04978949d42a5f70e0e28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_84762018def04806aa56a3b002062719",
      "max": 31,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3ba044f88fd64a39bf6867db26aa8c98",
      "value": 27
     }
    },
    "e05b9e80147749fcbda2a8b974116880": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_08b8dc02bd6342518d6c3ed06aad4196",
      "max": 3126,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f6789df63fff4945abf92e14ffe6cb99",
      "value": 3126
     }
    },
    "ebe65f02f66b4c19b7847c01af5e6d0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f519b956aa1b45a3b552977a7da173eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f6789df63fff4945abf92e14ffe6cb99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fb09d567dba94414b09751f20bc78fd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2b586dc188434998a6d47e984d1a7d8d",
      "placeholder": "​",
      "style": "IPY_MODEL_ab31e4467f5f4e70b536b8de0675ebb8",
      "value": " 3125/3126 [03:21&lt;00:00, 14.94it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
